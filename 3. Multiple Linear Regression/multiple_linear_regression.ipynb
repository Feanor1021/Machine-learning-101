{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CazISR8X_HUG"
   },
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "\n",
    "Multiple linear regression is used to estimate the relationship between two or more independent variables and one dependent variable. You can use multiple linear regression when you want to know:\n",
    "\n",
    "1. How strong the relationship is between two or more independent variables and one dependent variable (e.g. how rainfall, temperature, and amount of fertilizer added affect crop growth).\n",
    "\n",
    "2. The value of the dependent variable at a certain value of the independent variables (e.g. the expected yield of a crop at certain levels of rainfall, temperature, and fertilizer addition).\n",
    "\n",
    "It is just like the simple linear regression but in here corralation between the features and the dependent values are much more complex. Even though it is much more complex general structure of our code is not change much.\n",
    "\n",
    "When working with the multiple linear fetures there may be some problems. First, there may be any categorical value. As we know from before section, we have to encode our categorical datas. When we do that, if we hat 3 different categorical data we have to add 3 columns to our data set due to the onehotencoding method. If we put all the categorical encoded columns to our x_train data set there will be an error which is called as \"Dummy trap\"(https://www.geeksforgeeks.org/ml-dummy-variable-trap-in-regression-models/). We have to take care of this error it may cause of prediction errors. Another problem is that there may also be groups of data that are less statistically significant. To check this we have to look our data groups \"P\" value. To solve such problems we have several thecniques.\n",
    "\n",
    "1. All-in\n",
    "2. Backward Elimination\n",
    "3. Forward Elimination\n",
    "4. Bidirectional Elimination\n",
    "5. Score Comparison\n",
    "\n",
    "2, 3 and 4 are called as Stepwise Regression. Luckly scikit_learn module has class that LinearRegression which has amazing methods and tools that can solve the dummy trap and the last problem which we have spoken.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pOyqYHTk_Q57"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vgC61-ah_WIz"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"50_Startups.csv\")\n",
    "x = dataset.iloc[:, :-1].values # features\n",
    "y = dataset.iloc[:, -1].values # dependent values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VadrvE7s_lS9"
   },
   "source": [
    "## Encoding categorical data\n",
    "\n",
    "We have number of categorical values so we'll encode this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [3])], remainder=(\"passthrough\"))\n",
    "x = np.array(ct.fit_transform(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WemVnqgeA70k"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set\n",
    "\n",
    "In multiple linear regression the equation will be like y = b0 + b1X + b2X + b3X... As you may see all the features have it's own coefficient because of that we don't need to do feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k-McZVsQBINc"
   },
   "source": [
    "## Training the Multiple Linear Regression model on the Training set\n",
    "\n",
    "Numpy LinearRegression class has amazing tools and methods that can solve automatically \"dummy trap\" and can implement best feature to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xNkXL1YQBiBT"
   },
   "source": [
    "## Predicting the Test set results\n",
    "\n",
    "Since there are several features which has some correlation, we can't easily plot our data set's result. To see our models success we will print them together as a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103282.38 103015.2 ]\n",
      " [144259.4  132582.28]\n",
      " [146121.95 132447.74]\n",
      " [ 77798.83  71976.1 ]\n",
      " [191050.39 178537.48]\n",
      " [105008.31 116161.24]\n",
      " [ 81229.06  67851.69]\n",
      " [ 97483.56  98791.73]\n",
      " [110352.25 113969.44]\n",
      " [166187.94 167921.07]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = regressor.predict(x_test)\n",
    "np.set_printoptions(precision = 2) # precision after . is 2\n",
    "np.set_printoptions(suppress = True) #numpy array won't be printed with scientific notation\n",
    "# we will reshape and concatenate our y_test and y_pred vectors\n",
    "print(np.concatenate((y_test.reshape(len(y_test),1),y_pred.reshape(len(y_pred),1)), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In order to better see the success of our model, we will calculate how many percent wrong each prediction is and add it to our prediction matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.26]\n",
      " [  8.09]\n",
      " [  9.36]\n",
      " [  7.48]\n",
      " [  6.55]\n",
      " [-10.62]\n",
      " [ 16.47]\n",
      " [ -1.34]\n",
      " [ -3.28]\n",
      " [ -1.04]]\n"
     ]
    }
   ],
   "source": [
    "error = []\n",
    "for i in range(len(y_pred)):\n",
    "    error.append((y_test[i]-y_pred[i])*100/y_test[i])\n",
    "np_error = np.array(error).reshape(len(error),1)\n",
    "print(np_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103282.38 103015.2       0.26]\n",
      " [144259.4  132582.28      8.09]\n",
      " [146121.95 132447.74      9.36]\n",
      " [ 77798.83  71976.1       7.48]\n",
      " [191050.39 178537.48      6.55]\n",
      " [105008.31 116161.24    -10.62]\n",
      " [ 81229.06  67851.69     16.47]\n",
      " [ 97483.56  98791.73     -1.34]\n",
      " [110352.25 113969.44     -3.28]\n",
      " [166187.94 167921.07     -1.04]]\n"
     ]
    }
   ],
   "source": [
    "print(np.concatenate((y_test.reshape(len(y_test),1),y_pred.reshape(len(y_pred),1),np_error), 1))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPhYhte6t7H4wEK4xPpDWT7",
   "name": "Multiple Linear Regression",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
